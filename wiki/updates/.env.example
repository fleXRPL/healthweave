# ═══════════════════════════════════════════════════════════════
# HealthWeave Environment Configuration
# ═══════════════════════════════════════════════════════════════

# ───────────────────────────────────────────────────────────────
# AI Provider Configuration
# ───────────────────────────────────────────────────────────────
# Which AI provider to use: 'ollama' (free, local) or 'claude' (paid, cloud)
# 
# Development: Use 'ollama' for free local testing
# Production:  Use 'claude' for best quality medical analysis
AI_PROVIDER=ollama

# ───────────────────────────────────────────────────────────────
# Claude Configuration (Production)
# ───────────────────────────────────────────────────────────────
# Get your API key from: https://console.anthropic.com/
# Required when AI_PROVIDER=claude
ANTHROPIC_API_KEY=your_claude_api_key_here

# Claude model to use (recommended: claude-sonnet-4-20250514)
# Options:
#   - claude-opus-4-20250514       (Most capable, most expensive)
#   - claude-sonnet-4-20250514     (Best balance, recommended)
#   - claude-sonnet-4-5-20250929   (Newer sonnet version)
CLAUDE_MODEL=claude-sonnet-4-20250514

# ───────────────────────────────────────────────────────────────
# Ollama Configuration (Development)
# ───────────────────────────────────────────────────────────────
# Base URL for Ollama API (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use (recommended: llama3.1:70b)
# Options:
#   - llama3.1:70b          (Best quality for medical, 40GB RAM required)
#   - llama3.1:8b           (Faster, less accurate, 8GB RAM required)
#   - mistral:7b            (Fast, decent quality, 8GB RAM required)
#   - llama3.2:latest       (Latest Llama, good balance)
#
# Note: Download models with: ollama pull llama3.1:70b
OLLAMA_MODEL=llama3.1:70b

# ───────────────────────────────────────────────────────────────
# Application Configuration
# ───────────────────────────────────────────────────────────────
NODE_ENV=development
PORT=3001

# ───────────────────────────────────────────────────────────────
# AWS Configuration (Production)
# ───────────────────────────────────────────────────────────────
# For production deployment to AWS
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key

# S3 Bucket for document storage
S3_BUCKET_NAME=healthweave-documents

# DynamoDB Tables
DYNAMODB_REPORTS_TABLE=healthweave-reports
DYNAMODB_AUDIT_TABLE=healthweave-audit-logs

# ───────────────────────────────────────────────────────────────
# LocalStack Configuration (Local Development)
# ───────────────────────────────────────────────────────────────
# Uncomment to use LocalStack instead of real AWS
# LOCALSTACK_ENDPOINT=http://localhost:4566
# USE_LOCALSTACK=true

# ───────────────────────────────────────────────────────────────
# Logging Configuration
# ───────────────────────────────────────────────────────────────
LOG_LEVEL=debug

# ═══════════════════════════════════════════════════════════════
# Quick Setup Guide
# ═══════════════════════════════════════════════════════════════
#
# FOR DEVELOPMENT (Free, Local):
# 1. Set AI_PROVIDER=ollama
# 2. Install Ollama: https://ollama.ai/download
# 3. Pull a model: ollama pull llama3.1:70b
# 4. Start Ollama: ollama serve
# 5. You're ready to develop!
#
# FOR PRODUCTION (Paid, Cloud):
# 1. Set AI_PROVIDER=claude
# 2. Get API key from https://console.anthropic.com/
# 3. Set ANTHROPIC_API_KEY=your_key
# 4. Deploy to AWS with proper credentials
#
# ═══════════════════════════════════════════════════════════════
