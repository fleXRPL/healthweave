# AI Provider Setup Guide

This guide explains how to set up HealthWeave for development (free with Ollama) and production (paid with Claude).

## Table of Contents

1. [Quick Start: Development with Ollama](#development-setup-ollama-free)
2. [Production Setup: Claude](#production-setup-claude-paid)
3. [Model Recommendations](#model-recommendations)
4. [Quality Comparison](#quality-comparison)
5. [Switching Between Providers](#switching-between-providers)
6. [Troubleshooting](#troubleshooting)

---

## Development Setup: Ollama (FREE)

### Why Ollama for Development?

- ‚úÖ **Free** - No API costs during development/testing
- ‚úÖ **Local** - Works offline, no internet required
- ‚úÖ **Privacy** - Medical data never leaves your machine
- ‚úÖ **Fast iteration** - No rate limits, unlimited testing

‚ö†Ô∏è **Trade-offs:**

- Lower quality than Claude (70-80% vs 95% accuracy)
- Citations may be hallucinated (verify carefully)
- Requires significant RAM (8-40GB depending on model)

### Step 1: Install Ollama

**macOS / Linux:**

```bash
# Download and install from https://ollama.ai/download
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download installer from https://ollama.ai/download

### Step 2: Choose and Download a Model

**Recommended for HealthWeave:**

#### Option A: Llama 3.1 70B (BEST for medical)

```bash
ollama pull llama3.1:70b
```

- **RAM Required:** 40GB
- **Quality:** 8/10 for medical analysis
- **Citation Quality:** 6/10 (watch for hallucinations)
- **Speed:** Slow (5-10 minutes per analysis on CPU)
- **Best For:** When you need highest quality Ollama results

#### Option B: Llama 3.1 8B (FASTER, good enough)

```bash
ollama pull llama3.1:8b
```

- **RAM Required:** 8GB
- **Quality:** 6/10 for medical analysis
- **Citation Quality:** 5/10
- **Speed:** Fast (30-60 seconds per analysis)
- **Best For:** Quick iteration, prompt testing

#### Option C: Llama 3.2 Latest (BALANCED)

```bash
ollama pull llama3.2:latest
```

- **RAM Required:** 8GB
- **Quality:** 7/10 for medical analysis
- **Citation Quality:** 5/10
- **Speed:** Fast (30-60 seconds)
- **Best For:** General development when 70B is too slow

**Not Recommended:**

- `mistral:7b` - Struggles with medical terminology
- `codellama` - Optimized for code, not medical analysis

### Step 3: Start Ollama Server

```bash
# Start Ollama service
ollama serve
```

Keep this terminal window open. You should see:

```
Ollama is running at http://localhost:11434
```

### Step 4: Configure HealthWeave

1. Copy environment template:

```bash
cd backend
cp .env.example .env
```

2. Edit `.env`:

```bash
AI_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:70b
```

3. Test connection:

```bash
npm run dev
```

### Step 5: Verify Setup

Test that Ollama is working:

```bash
# In a new terminal
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:70b",
  "prompt": "Say hello",
  "stream": false
}'
```

You should get a JSON response with Llama's greeting.

---

## Production Setup: Claude (PAID)

### Why Claude for Production?

‚úÖ **Highest Quality** - 95%+ accuracy on medical synthesis
‚úÖ **Reliable Citations** - Rarely hallucinates references
‚úÖ **Consistent** - Reproducible results across runs
‚úÖ **No Infrastructure** - Serverless, scales automatically
‚úÖ **Fast** - 30-60 seconds per analysis

üí∞ **Costs:**

- **Claude Sonnet 4:** ~$0.015 per analysis
- **Claude Opus 4:** ~$0.075 per analysis
- For 1000 analyses/month: $15-75/month

### Step 1: Get Claude API Key

1. Go to https://console.anthropic.com/
2. Sign up or log in
3. Navigate to API Keys section
4. Create a new API key
5. Copy the key (starts with `sk-ant-...`)

‚ö†Ô∏è **Keep your API key secret!** Never commit it to git.

### Step 2: Configure HealthWeave

1. Edit `.env`:

```bash
AI_PROVIDER=claude
ANTHROPIC_API_KEY=sk-ant-your-actual-key-here
CLAUDE_MODEL=claude-sonnet-4-20250514
```

2. Restart backend:

```bash
npm run dev
```

### Step 3: Verify Setup

Check logs for:

```
AI Service initialized with provider: claude
  Model: claude-sonnet-4-20250514
```

---

## Model Recommendations

### For HealthWeave Development

**Primary Choice:**

```bash
AI_PROVIDER=ollama
OLLAMA_MODEL=llama3.1:70b
```

Best balance of quality and cost for development.

**If RAM Limited (<40GB):**

```bash
OLLAMA_MODEL=llama3.1:8b
```

Good enough for prompt testing and iteration.

### For HealthWeave Production

**Primary Choice:**

```bash
AI_PROVIDER=claude
CLAUDE_MODEL=claude-sonnet-4-20250514
```

Best balance of quality and cost for production.

**If Budget Unlimited:**

```bash
CLAUDE_MODEL=claude-opus-4-20250514
```

Absolute highest quality, but 5x more expensive.

---

## Quality Comparison

### Medical Analysis Quality

| Model               | Medical Accuracy | Citation Accuracy | Cross-Specialty Synthesis | Cost |
| ------------------- | ---------------- | ----------------- | ------------------------- | ---- |
| **Claude Opus 4**   | 98%              | 95%               | Excellent                 | $$$$ |
| **Claude Sonnet 4** | 95%              | 92%               | Excellent                 | $$   |
| **Llama 3.1 70B**   | 80%              | 60% ‚ö†Ô∏è            | Good                      | Free |
| **Llama 3.1 8B**    | 65%              | 50% ‚ö†Ô∏è            | Fair                      | Free |
| **Llama 3.2**       | 70%              | 55% ‚ö†Ô∏è            | Fair                      | Free |

‚ö†Ô∏è = Watch for hallucinated citations

### Real-World Example

**Test Case:** Patient with BRCA1 variant, on multiple cardiac medications, family history of cancer

**Claude Sonnet 4 Output:**

```json
{
  "finding": "BRCA1 c.5266dupC pathogenic variant",
  "citations": [
    {
      "source": "ClinVar",
      "reference": "VCV000128620",
      "relevance": "Variant classification per ACMG criteria"
    },
    {
      "source": "PubMed",
      "reference": "PMID: 17924331",
      "relevance": "45-87% lifetime breast cancer risk"
    }
  ]
}
```

‚úÖ Both citations are REAL and accurate

**Llama 3.1 70B Output:**

```json
{
  "finding": "BRCA1 c.5266dupC pathogenic variant",
  "citations": [
    {
      "source": "ClinVar",
      "reference": "VCV000128620",
      "relevance": "Variant classification"
    },
    {
      "source": "Journal of Medical Genetics",
      "reference": "Vol 45, 2008, pp 234-240",
      "relevance": "Breast cancer risk assessment"
    }
  ]
}
```

‚úÖ First citation is real
‚ö†Ô∏è Second citation is HALLUCINATED (journal vol/pages don't exist)

**Verdict:** Ollama is fine for development, but ALWAYS use Claude for production medical analysis.

---

## Switching Between Providers

### Development ‚Üí Production

1. **Update .env:**

```bash
# Change this:
AI_PROVIDER=ollama

# To this:
AI_PROVIDER=claude
ANTHROPIC_API_KEY=your_actual_key
```

2. **Restart backend:**

```bash
npm run dev
```

That's it! No code changes needed.

### A/B Testing

Want to compare outputs?

1. **Save Ollama output:**

```bash
AI_PROVIDER=ollama npm run analyze > ollama-results.json
```

2. **Save Claude output:**

```bash
AI_PROVIDER=claude npm run analyze > claude-results.json
```

3. **Compare:**

```bash
diff ollama-results.json claude-results.json
```

---

## Troubleshooting

### Ollama Issues

**Problem:** `Cannot connect to Ollama at http://localhost:11434`

**Solution:**

```bash
# Check if Ollama is running
ps aux | grep ollama

# If not, start it
ollama serve
```

**Problem:** `Model 'llama3.1:70b' not found`

**Solution:**

```bash
# Download the model
ollama pull llama3.1:70b

# List available models
ollama list
```

**Problem:** Ollama is really slow

**Solutions:**

- Use smaller model: `llama3.1:8b` instead of `70b`
- Use GPU if available (automatic with NVIDIA GPU)
- Reduce `num_predict` in `ai.service.ts` (generates less text)

**Problem:** Out of memory

**Solution:**

```bash
# Use smaller model
ollama pull llama3.1:8b

# Update .env
OLLAMA_MODEL=llama3.1:8b
```

### Claude Issues

**Problem:** `Claude API key required when using Claude provider`

**Solution:**
Check your `.env` file has:

```bash
ANTHROPIC_API_KEY=sk-ant-your-actual-key
```

**Problem:** `429 Rate Limit Exceeded`

**Solution:**
You're making too many requests. Wait 60 seconds or upgrade your Claude plan.

**Problem:** `400 Bad Request - messages: field required`

**Solution:**
Check that your prompt isn't empty. Enable debug logging:

```bash
LOG_LEVEL=debug npm run dev
```

### General Issues

**Problem:** Analysis returns gibberish

**Possible Causes:**

1. Ollama model is hallucinating (switch to Claude)
2. Prompt has unsubstituted variables (check logs)
3. AI returned non-JSON (check raw response in logs)

**Solution:**

```bash
# Enable debug logging
LOG_LEVEL=debug npm run dev

# Check prompt is loading correctly
# Look for: "Prompt tokens (estimated): XXXX"
```

---

## Development Workflow

### Recommended Workflow

**Phase 1: Prompt Development (Ollama)**

- Use `llama3.1:8b` for fast iteration
- Test prompt changes quickly
- Focus on structure and output format
- Don't worry about citation quality yet

**Phase 2: Quality Refinement (Ollama 70B)**

- Switch to `llama3.1:70b` for better quality
- Validate medical reasoning
- Check that cross-specialty connections work
- Citations will still have issues (expected)

**Phase 3: Production Testing (Claude)**

- Switch to `claude-sonnet-4`
- Validate citation accuracy
- Run on real test cases
- Compare to genetic counselor review

**Phase 4: Production Deployment (Claude)**

- Keep `AI_PROVIDER=claude` in production
- Monitor costs and usage
- Set up alerts for API errors

### Cost Management

**Development:** Free (Ollama)
**Testing:** ~$1-5 (Claude on 100 test analyses)
**Production:** ~$15-75/month (Claude on 1000-5000 analyses)

**To minimize costs:**

- Do ALL prompt development with Ollama
- Only use Claude for final validation
- Use Claude Sonnet (not Opus) unless quality demands it
- Implement caching for repeated analyses

---

## System Requirements

### For Ollama Development

**Minimum (8B models):**

- 8GB RAM
- 10GB disk space
- Modern CPU (any)

**Recommended (70B models):**

- 40GB RAM
- 100GB disk space
- GPU with 24GB VRAM (optional, but much faster)

**Optimal (70B models with GPU):**

- 64GB RAM
- 100GB disk space
- NVIDIA GPU with 48GB VRAM (A6000, A100)

### For Claude Production

**Any computer with internet** - Claude runs in the cloud!

---

## Next Steps

1. ‚úÖ Choose your AI provider (Ollama for dev, Claude for prod)
2. ‚úÖ Follow the setup steps above
3. ‚úÖ Test with sample documents
4. ‚úÖ Validate output quality
5. ‚úÖ Iterate on prompts
6. ‚úÖ Deploy to production with Claude

**Questions?** Open an issue on GitHub or contact the team.

---

**Last Updated:** 2025-01-15
**Maintained By:** HealthWeave Engineering Team
